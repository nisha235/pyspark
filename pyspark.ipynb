{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjILla5Oxm2utZerBI2E/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisha235/pyspark/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnt8dYzOw0T1",
        "outputId": "fcb28541-c911-4490-e1e7-642db37a68b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=d8fcd096ff8c1680981ba6136ce7a984697d4cd196b3d894137855854219e3ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 1\n",
        "#Get the individual key from nested dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "U8YKdbPp7K0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExpandNestedData\").getOrCreate()\n",
        "\n",
        "# Sample data as a list of tuples\n",
        "data = [(10, [{'k1':20,'k2':40},{'k1':60, 'k2':80}]),(20, [{'k1':30,'k2':50},{'k1':90, 'k2':90}]) ]\n",
        "df = spark.createDataFrame(data, [\"col1\", \"nested_data\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu9qqnQ5xQif",
        "outputId": "4178365e-c217-418b-e601-3703097585e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "|col1|         nested_data|\n",
            "+----+--------------------+\n",
            "|  10|[{k1 -> 20, k2 ->...|\n",
            "|  20|[{k1 -> 30, k2 ->...|\n",
            "+----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Explode the nested list\n",
        "exploded_df = df.withColumn(\"data\", explode(\"nested_data\"))\n",
        "exploded_df.show()\n",
        "# Select and rename columns\n",
        "poutput = exploded_df.select(\"col1\", \"data.k1\", \"data.k2\")\n",
        "poutput.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyEbj1YuxqL5",
        "outputId": "b077ecad-ca00-45f3-beb2-b85d95ddc9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+--------------------+\n",
            "|col1|         nested_data|                data|\n",
            "+----+--------------------+--------------------+\n",
            "|  10|[{k1 -> 20, k2 ->...|{k1 -> 20, k2 -> 40}|\n",
            "|  10|[{k1 -> 20, k2 ->...|{k1 -> 60, k2 -> 80}|\n",
            "|  20|[{k1 -> 30, k2 ->...|{k1 -> 30, k2 -> 50}|\n",
            "|  20|[{k1 -> 30, k2 ->...|{k1 -> 90, k2 -> 90}|\n",
            "+----+--------------------+--------------------+\n",
            "\n",
            "+----+---+---+\n",
            "|col1| k1| k2|\n",
            "+----+---+---+\n",
            "|  10| 20| 40|\n",
            "|  10| 60| 80|\n",
            "|  20| 30| 50|\n",
            "|  20| 90| 90|\n",
            "+----+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 2\n",
        "Exploding list of list"
      ],
      "metadata": {
        "id": "UuWAghJ369pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, array_max, col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExplodeNestedArray\").getOrCreate()\n",
        "\n",
        "# Sample data as a list of tuples\n",
        "data = [([[[1,2,3], [2,3,4],[3,4,5,6]]])]\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = spark.createDataFrame(data, [\"nested_data\"])\n",
        "df.show()\n",
        "# Explode each nested list\n",
        "exploded_df = df.withColumn(\"data\", explode(\"nested_data\"))\n",
        "exploded_df.show()\n",
        "\n",
        "exploded_df = exploded_df.withColumn('max', array_max(col('data')))\n",
        "row1 = exploded_df.agg({\"max\": \"max\"}).collect()[0]\n",
        "print(row1)\n",
        "# Select and rename columns\n",
        "exploded_df.select([expr('data[' + str(x) + ']') for x in range(0, 4)]).show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtgix5U_y5Km",
        "outputId": "470f589e-16e6-428a-a10f-cc2ca79afc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|         nested_data|\n",
            "+--------------------+\n",
            "|[[1, 2, 3], [2, 3...|\n",
            "+--------------------+\n",
            "\n",
            "+--------------------+------------+\n",
            "|         nested_data|        data|\n",
            "+--------------------+------------+\n",
            "|[[1, 2, 3], [2, 3...|   [1, 2, 3]|\n",
            "|[[1, 2, 3], [2, 3...|   [2, 3, 4]|\n",
            "|[[1, 2, 3], [2, 3...|[3, 4, 5, 6]|\n",
            "+--------------------+------------+\n",
            "\n",
            "Row(max(max)=6)\n",
            "+-------+-------+-------+-------+\n",
            "|data[0]|data[1]|data[2]|data[3]|\n",
            "+-------+-------+-------+-------+\n",
            "|      1|      2|      3|   NULL|\n",
            "|      2|      3|      4|   NULL|\n",
            "|      3|      4|      5|      6|\n",
            "+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 3\n",
        "Get the frequency of unique values in the entire dataframe df.\n"
      ],
      "metadata": {
        "id": "O6CNmj_n66SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUES 3\n",
        "#Get the frequency of unique values in the entire dataframe df.\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "spark = SparkSession.builder.appName(\"frequency\").getOrCreate()\n",
        "\n",
        "data = [(1, 2, 3),\n",
        "(2, 3, 4),\n",
        "(1, 2, 3),\n",
        "(4, 5, 6),\n",
        "(2, 3, 4)]\n",
        "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
        "\n",
        "\n",
        "# Print DataFrame\n",
        "df.show()\n",
        "# get column names\n",
        "columns = df.columns\n",
        "\n",
        "\n",
        "# stack all columns into a single column\n",
        "df_single = None\n",
        "\n",
        "\n",
        "for c in columns:\n",
        "  if df_single is None:\n",
        "    df_single = df.select(col(c).alias(\"single_column\"))\n",
        "  else:\n",
        "    df_single = df_single.union(df.select(col(c).alias(\"single_column\")))\n",
        "\n",
        "\n",
        "# generate frequency table\n",
        "frequency_table = df_single.groupBy(\"single_column\").count().orderBy('count', ascending=False)\n",
        "\n",
        "\n",
        "# show frequency table\n",
        "frequency_table.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tBVhvGQp0Ohi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0317cc-3f08-40e1-cd33-8092a729de44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+\n",
            "|Column1|Column2|Column3|\n",
            "+-------+-------+-------+\n",
            "|      1|      2|      3|\n",
            "|      2|      3|      4|\n",
            "|      1|      2|      3|\n",
            "|      4|      5|      6|\n",
            "|      2|      3|      4|\n",
            "+-------+-------+-------+\n",
            "\n",
            "+-------------+-----+\n",
            "|single_column|count|\n",
            "+-------------+-----+\n",
            "|            2|    4|\n",
            "|            3|    4|\n",
            "|            4|    3|\n",
            "|            1|    2|\n",
            "|            5|    1|\n",
            "|            6|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUE 4\n",
        "Appplication of REGEX_EXTRACT and REPPLACE, EXPPR AND SELECT"
      ],
      "metadata": {
        "id": "oMPzApUn-iEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_extract, col, expr, split, element_at\n",
        "\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "   .appName(\"RegExp and Replace Example\") \\\n",
        "   .getOrCreate()\n",
        "\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"mkt_id: 736482|user-click|landing_page\",),\n",
        "       (\"mkt_id: 839274|user-view|homepage\",),\n",
        "       (\"mkt_id: 128374|user-click|product_page\",)]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data, [\"mkt_code\"])\n",
        "df.show(truncate=False)\n",
        "\n",
        "\n",
        "# Apply transformations\n",
        "transformed_df = df.withColumn(\"product_marketing_id\",\n",
        "                              expr(\"REPLACE(REGEXP_EXTRACT(mkt_code, 'mkt_id:[^|]*', 0), 'mkt_id:', '')\"))\n",
        "\n",
        "# Extract the user-click value using regexp_extract\n",
        "\n",
        "\n",
        "#transformed_df = transformed_df.withColumn(\"product_view\",regexp_extract(col(\"mkt_code\"), '\\|(.*?)\\|', 1))\n",
        "#transformed_df = transformed_df.withColumn(\"product_page\",regexp_extract(col(\"mkt_code\"), '\\|[-_a-z]+\\|(.*?)$', 1))\n",
        "# Show transformed DataFrame\n",
        "#transformed_df.show(truncate=False)\n",
        "split_df = transformed_df.withColumn(\"split_array\", split(\"mkt_code\", \"\\|\"))\n",
        "\n",
        "# Extract the landing_page value using element_at (last element)\n",
        "extracted_df = split_df.withColumn(\"product_view\", element_at(\"split_array\", 2))\n",
        "extracted_df = extracted_df.withColumn(\"product_page\", element_at(\"split_array\", -1))  # -1 for last element\n",
        "\n",
        "# Show transformed DataFrame\n",
        "extracted_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlTUeNnG6aKA",
        "outputId": "5bd20586-62f4-451f-f2ae-8db7f67e7764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------+\n",
            "|mkt_code                              |\n",
            "+--------------------------------------+\n",
            "|mkt_id: 736482|user-click|landing_page|\n",
            "|mkt_id: 839274|user-view|homepage     |\n",
            "|mkt_id: 128374|user-click|product_page|\n",
            "+--------------------------------------+\n",
            "\n",
            "+--------------------------------------+--------------------+------------------------------------------+------------+------------+\n",
            "|mkt_code                              |product_marketing_id|split_array                               |product_view|product_page|\n",
            "+--------------------------------------+--------------------+------------------------------------------+------------+------------+\n",
            "|mkt_id: 736482|user-click|landing_page| 736482             |[mkt_id: 736482, user-click, landing_page]|user-click  |landing_page|\n",
            "|mkt_id: 839274|user-view|homepage     | 839274             |[mkt_id: 839274, user-view, homepage]     |user-view   |homepage    |\n",
            "|mkt_id: 128374|user-click|product_page| 128374             |[mkt_id: 128374, user-click, product_page]|user-click  |product_page|\n",
            "+--------------------------------------+--------------------+------------------------------------------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que 5\n",
        "To find the start and end flight from the dataset"
      ],
      "metadata": {
        "id": "Qn_OGQguNkCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateDataFrameExample\").getOrCreate()\n",
        "\n",
        "data = [ ('c1', 'New York', 'Lima'),\n",
        "   ('c1', 'London', 'New York'),\n",
        "   ('c1', 'Lima', 'Sao Paulo'),\n",
        "   ('c1', 'Sao Paulo', 'New Delhi'),\n",
        "   ('c2', 'Mumbai', 'Hyderabad'),\n",
        "   ('c2', 'Surat', 'Pune'),\n",
        "   ('c2', 'Hyderabad', 'Surat'),\n",
        "   ('c3', 'Kochi', 'Kurnool'),\n",
        "   ('c3', 'Lucknow', 'Agra'),\n",
        "   ('c3', 'Agra', 'Jaipur'),\n",
        "   ('c3', 'Jaipur', 'Kochi')]\n",
        "\n",
        "schema = \"customer string , start_location string , end_location string\"\n",
        "df = spark.createDataFrame(data = data , schema = schema)\n",
        "df.show()\n",
        "\n",
        "df_start = df.select(col(\"customer\"), col(\"start_location\").alias(\"loc\"))\n",
        "df_end = df.select(col(\"customer\"), col(\"end_location\").alias(\"loc\"))\n",
        "\n",
        "df_union = df_start.union(df_end)\n",
        "\n",
        "df_unique = df_union.groupBy(col(\"customer\") , col(\"loc\")).agg(\n",
        "   count(lit(1)).alias(\"cn\")\n",
        ").filter(col(\"cn\") ==1).drop(col(\"cn\")).orderBy(col(\"customer\"))\n",
        "\n",
        "df_unique.show()\n",
        "df_answer = df.join(df_unique, on= ((df.customer == df_unique.customer) &(df.start_location == df_unique.loc) | (df.end_location == df_unique.loc)), how='inner').drop(df.customer)\n",
        "df_answer = df_answer.withColumn('cust_start_loc', when(col('start_location')==col('loc'), col('start_location'))).withColumn('cust_end_loc', when(col('end_location')==col('loc'), col('end_location'))).select(col(\"customer\"), col(\"cust_start_loc\"), col(\"cust_end_loc\"))\n",
        "df_answer.show()\n",
        "df_answer = df_answer.groupby(col('customer')).agg(min(col('cust_start_loc')).alias('start_loation'),min(col('cust_end_loc')).alias('end_loation'))\n",
        "df_answer.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwQ61-Xo-psB",
        "outputId": "a24d1a53-b790-44c9-88e7-ff184b27de44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------+------------+\n",
            "|customer|start_location|end_location|\n",
            "+--------+--------------+------------+\n",
            "|      c1|      New York|        Lima|\n",
            "|      c1|        London|    New York|\n",
            "|      c1|          Lima|   Sao Paulo|\n",
            "|      c1|     Sao Paulo|   New Delhi|\n",
            "|      c2|        Mumbai|   Hyderabad|\n",
            "|      c2|         Surat|        Pune|\n",
            "|      c2|     Hyderabad|       Surat|\n",
            "|      c3|         Kochi|     Kurnool|\n",
            "|      c3|       Lucknow|        Agra|\n",
            "|      c3|          Agra|      Jaipur|\n",
            "|      c3|        Jaipur|       Kochi|\n",
            "+--------+--------------+------------+\n",
            "\n",
            "+--------+---------+\n",
            "|customer|      loc|\n",
            "+--------+---------+\n",
            "|      c1|   London|\n",
            "|      c1|New Delhi|\n",
            "|      c2|   Mumbai|\n",
            "|      c2|     Pune|\n",
            "|      c3|  Lucknow|\n",
            "|      c3|  Kurnool|\n",
            "+--------+---------+\n",
            "\n",
            "+--------+--------------+------------+\n",
            "|customer|cust_start_loc|cust_end_loc|\n",
            "+--------+--------------+------------+\n",
            "|      c1|        London|        NULL|\n",
            "|      c1|          NULL|   New Delhi|\n",
            "|      c2|        Mumbai|        NULL|\n",
            "|      c2|          NULL|        Pune|\n",
            "|      c3|          NULL|     Kurnool|\n",
            "|      c3|       Lucknow|        NULL|\n",
            "+--------+--------------+------------+\n",
            "\n",
            "+--------+-------------+-----------+\n",
            "|customer|start_loation|end_loation|\n",
            "+--------+-------------+-----------+\n",
            "|      c1|       London|  New Delhi|\n",
            "|      c2|       Mumbai|       Pune|\n",
            "|      c3|      Lucknow|    Kurnool|\n",
            "+--------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que6\n",
        "Replace both the diagonals of dataframe with 0"
      ],
      "metadata": {
        "id": "OLjt69VnO1m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a numeric DataFrame\n",
        "data = [(1, 2, 3, 4),\n",
        "(2, 3, 4, 5),\n",
        "(1, 2, 3, 4),\n",
        "(4, 5, 6, 7)]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
        "\n",
        "\n",
        "# Print DataFrame\n",
        "df.show()\n",
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "\n",
        "# Define window specification\n",
        "w = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "\n",
        "# Add index\n",
        "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
        "\n",
        "\n",
        "df = df.select([when(col(\"id\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
        "\n",
        "\n",
        "# Create a reverse id column\n",
        "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
        "df = df.withColumn(\"id_2\", df.count() - 1 - df[\"id\"])\n",
        "\n",
        "\n",
        "df_with_diag_zero = df.select([when(col(\"id_2\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
        "\n",
        "\n",
        "df_with_diag_zero.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dghDBIwfH-Qb",
        "outputId": "aa6844c6-e6ad-4ed3-80a8-e535ac622f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+-----+\n",
            "|col_1|col_2|col_3|col_4|\n",
            "+-----+-----+-----+-----+\n",
            "|    1|    2|    3|    4|\n",
            "|    2|    3|    4|    5|\n",
            "|    1|    2|    3|    4|\n",
            "|    4|    5|    6|    7|\n",
            "+-----+-----+-----+-----+\n",
            "\n",
            "+-----+-----+-----+-----+\n",
            "|col_1|col_2|col_3|col_4|\n",
            "+-----+-----+-----+-----+\n",
            "|    0|    2|    3|    0|\n",
            "|    2|    0|    0|    5|\n",
            "|    1|    0|    0|    4|\n",
            "|    0|    5|    6|    0|\n",
            "+-----+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUE-7 Flatten the Nested Structure with Array and Struct\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Y02vSPwBHhTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "from pyspark.sql.functions import explode_outer, col\n",
        "# Define address struct\n",
        "address_struct = StructType([\n",
        "  StructField(\"street\", StringType(), True),\n",
        "  StructField(\"city\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Define person struct (with nested address)\n",
        "person_struct = StructType([\n",
        "  StructField(\"name\", StringType(), True),\n",
        "  StructField(\"age\", IntegerType(), True),\n",
        "  StructField(\"hobbies\", ArrayType(StringType())),\n",
        "  StructField(\"address\", address_struct)  # Nested address struct\n",
        "])\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.appName(\"NestedStructExample\").getOrCreate()\n",
        "\n",
        "# Schema with nested struct\n",
        "schema = StructType([\n",
        "  StructField(\"id\", IntegerType(), True),\n",
        "  StructField(\"data\", person_struct)  # Use the defined person_struct\n",
        "])\n",
        "data = [\n",
        "  (1, {\"name\": \"Alice\", \"age\": 30,  \"hobbies\": [\"music\", \"reading\"], \"address\": {\"street\": \"Main St\", \"city\": \"New York\"}}),\n",
        "  (2, {\"name\": \"Bob\", \"age\": 25,  \"hobbies\": [\"music\", \"dancing\"],  \"address\": {\"street\": \"Baker St\", \"city\": \"London\"}})\n",
        "]\n",
        "\n",
        "# Create DataFrame (sample data omitted for brevity)\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)\n",
        "print(df.schema.fields)\n",
        "# for field in df.schema.fields:\n",
        "#   if type(field.dataType) == ArrayType or type(field.dataType) == StructType:\n",
        "#     print('inside',field)\n",
        "#     complex_fields = dict([(field.name, field.dataType)])\n",
        "# print(complex_fields)\n",
        "#df = df.select(\"id\", df.data.getField(\"name\").alias(\"name\"), df.data.address.street)\n",
        "print(df.show())\n",
        "complex_fields = {'address':address_struct, 'data':person_struct}\n",
        "expanded = [col(\"data\"+'.'+k).alias(k) for k in [ n.name for n in complex_fields[\"data\"]]]\n",
        "df=df.select(\"*\", *expanded)\n",
        "#.drop(\"data.address\")\n",
        "df=df.withColumn(\"hobby\",explode_outer(\"hobbies\"))\n",
        "\n",
        "#for n in complex_field[\"address\"]:\n",
        "#    print(n.name)\n",
        "expanded = [col(\"address\"+'.'+k).alias(k) for k in [ n.name for n in complex_fields[\"address\"]]]\n",
        "print(expanded)\n",
        "df=df.select(\"*\", *expanded)\n",
        "df.drop('address', 'hobbies', 'data').show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "4MVOu1NSOw9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "766bce52-8e39-4397-daeb-2f94cb3db0da"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------------------------+\n",
            "|id |data                                              |\n",
            "+---+--------------------------------------------------+\n",
            "|1  |{Alice, 30, [music, reading], {Main St, New York}}|\n",
            "|2  |{Bob, 25, [music, dancing], {Baker St, London}}   |\n",
            "+---+--------------------------------------------------+\n",
            "\n",
            "[StructField('id', IntegerType(), True), StructField('data', StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('hobbies', ArrayType(StringType(), True), True), StructField('address', StructType([StructField('street', StringType(), True), StructField('city', StringType(), True)]), True)]), True)]\n",
            "+---+--------------------+\n",
            "| id|                data|\n",
            "+---+--------------------+\n",
            "|  1|{Alice, 30, [musi...|\n",
            "|  2|{Bob, 25, [music,...|\n",
            "+---+--------------------+\n",
            "\n",
            "None\n",
            "[Column<'address.street AS street'>, Column<'address.city AS city'>]\n",
            "+---+-----+---+-------+--------+--------+\n",
            "| id| name|age|  hobby|  street|    city|\n",
            "+---+-----+---+-------+--------+--------+\n",
            "|  1|Alice| 30|  music| Main St|New York|\n",
            "|  1|Alice| 30|reading| Main St|New York|\n",
            "|  2|  Bob| 25|  music|Baker St|  London|\n",
            "|  2|  Bob| 25|dancing|Baker St|  London|\n",
            "+---+-----+---+-------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJtvFOvSI0a1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}