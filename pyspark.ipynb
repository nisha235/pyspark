{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP29WUapaMHR4XXsNgXpcNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisha235/pyspark/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnt8dYzOw0T1",
        "outputId": "972afd59-896b-4984-ad96-aa524b4a3c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=01ffcd0a0a3aa1152cfd9ad313eafcc7d3824d6daff07054576375883f3e5ca7\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 1\n",
        "#Get the individual key from nested dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "U8YKdbPp7K0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExpandNestedData\").getOrCreate()\n",
        "\n",
        "# Sample data as a list of tuples\n",
        "data = [(10, [{'k1':20,'k2':40},{'k1':60, 'k2':80}]),(20, [{'k1':30,'k2':50},{'k1':90, 'k2':90}]) ]\n",
        "df = spark.createDataFrame(data, [\"col1\", \"nested_data\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu9qqnQ5xQif",
        "outputId": "4178365e-c217-418b-e601-3703097585e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "|col1|         nested_data|\n",
            "+----+--------------------+\n",
            "|  10|[{k1 -> 20, k2 ->...|\n",
            "|  20|[{k1 -> 30, k2 ->...|\n",
            "+----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Explode the nested list\n",
        "exploded_df = df.withColumn(\"data\", explode(\"nested_data\"))\n",
        "exploded_df.show()\n",
        "# Select and rename columns\n",
        "poutput = exploded_df.select(\"col1\", \"data.k1\", \"data.k2\")\n",
        "poutput.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyEbj1YuxqL5",
        "outputId": "b077ecad-ca00-45f3-beb2-b85d95ddc9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+--------------------+\n",
            "|col1|         nested_data|                data|\n",
            "+----+--------------------+--------------------+\n",
            "|  10|[{k1 -> 20, k2 ->...|{k1 -> 20, k2 -> 40}|\n",
            "|  10|[{k1 -> 20, k2 ->...|{k1 -> 60, k2 -> 80}|\n",
            "|  20|[{k1 -> 30, k2 ->...|{k1 -> 30, k2 -> 50}|\n",
            "|  20|[{k1 -> 30, k2 ->...|{k1 -> 90, k2 -> 90}|\n",
            "+----+--------------------+--------------------+\n",
            "\n",
            "+----+---+---+\n",
            "|col1| k1| k2|\n",
            "+----+---+---+\n",
            "|  10| 20| 40|\n",
            "|  10| 60| 80|\n",
            "|  20| 30| 50|\n",
            "|  20| 90| 90|\n",
            "+----+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 2\n",
        "Exploding list of list"
      ],
      "metadata": {
        "id": "UuWAghJ369pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, array_max, col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExplodeNestedArray\").getOrCreate()\n",
        "\n",
        "# Sample data as a list of tuples\n",
        "data = [([[[1,2,3], [2,3,4],[3,4,5,6]]])]\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = spark.createDataFrame(data, [\"nested_data\"])\n",
        "df.show()\n",
        "# Explode each nested list\n",
        "exploded_df = df.withColumn(\"data\", explode(\"nested_data\"))\n",
        "exploded_df.show()\n",
        "\n",
        "exploded_df = exploded_df.withColumn('max', array_max(col('data')))\n",
        "row1 = exploded_df.agg({\"max\": \"max\"}).collect()[0]\n",
        "print(row1)\n",
        "# Select and rename columns\n",
        "exploded_df.select([expr('data[' + str(x) + ']') for x in range(0, 4)]).show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtgix5U_y5Km",
        "outputId": "470f589e-16e6-428a-a10f-cc2ca79afc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|         nested_data|\n",
            "+--------------------+\n",
            "|[[1, 2, 3], [2, 3...|\n",
            "+--------------------+\n",
            "\n",
            "+--------------------+------------+\n",
            "|         nested_data|        data|\n",
            "+--------------------+------------+\n",
            "|[[1, 2, 3], [2, 3...|   [1, 2, 3]|\n",
            "|[[1, 2, 3], [2, 3...|   [2, 3, 4]|\n",
            "|[[1, 2, 3], [2, 3...|[3, 4, 5, 6]|\n",
            "+--------------------+------------+\n",
            "\n",
            "Row(max(max)=6)\n",
            "+-------+-------+-------+-------+\n",
            "|data[0]|data[1]|data[2]|data[3]|\n",
            "+-------+-------+-------+-------+\n",
            "|      1|      2|      3|   NULL|\n",
            "|      2|      3|      4|   NULL|\n",
            "|      3|      4|      5|      6|\n",
            "+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUES 3\n",
        "Get the frequency of unique values in the entire dataframe df.\n"
      ],
      "metadata": {
        "id": "O6CNmj_n66SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUES 3\n",
        "#Get the frequency of unique values in the entire dataframe df.\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "spark = SparkSession.builder.appName(\"frequency\").getOrCreate()\n",
        "\n",
        "data = [(1, 2, 3),\n",
        "(2, 3, 4),\n",
        "(1, 2, 3),\n",
        "(4, 5, 6),\n",
        "(2, 3, 4)]\n",
        "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
        "\n",
        "\n",
        "# Print DataFrame\n",
        "df.show()\n",
        "# get column names\n",
        "columns = df.columns\n",
        "\n",
        "\n",
        "# stack all columns into a single column\n",
        "df_single = None\n",
        "\n",
        "\n",
        "for c in columns:\n",
        "  if df_single is None:\n",
        "    df_single = df.select(col(c).alias(\"single_column\"))\n",
        "  else:\n",
        "    df_single = df_single.union(df.select(col(c).alias(\"single_column\")))\n",
        "\n",
        "\n",
        "# generate frequency table\n",
        "frequency_table = df_single.groupBy(\"single_column\").count().orderBy('count', ascending=False)\n",
        "\n",
        "\n",
        "# show frequency table\n",
        "frequency_table.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tBVhvGQp0Ohi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0317cc-3f08-40e1-cd33-8092a729de44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+\n",
            "|Column1|Column2|Column3|\n",
            "+-------+-------+-------+\n",
            "|      1|      2|      3|\n",
            "|      2|      3|      4|\n",
            "|      1|      2|      3|\n",
            "|      4|      5|      6|\n",
            "|      2|      3|      4|\n",
            "+-------+-------+-------+\n",
            "\n",
            "+-------------+-----+\n",
            "|single_column|count|\n",
            "+-------------+-----+\n",
            "|            2|    4|\n",
            "|            3|    4|\n",
            "|            4|    3|\n",
            "|            1|    2|\n",
            "|            5|    1|\n",
            "|            6|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUE 4\n",
        "Appplication of REGEX_EXTRACT and REPPLACE, EXPPR AND SELECT"
      ],
      "metadata": {
        "id": "oMPzApUn-iEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_extract, col, expr, split, element_at\n",
        "\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "   .appName(\"RegExp and Replace Example\") \\\n",
        "   .getOrCreate()\n",
        "\n",
        "\n",
        "# Sample DataFrame\n",
        "data = [(\"mkt_id: 736482|user-click|landing_page\",),\n",
        "       (\"mkt_id: 839274|user-view|homepage\",),\n",
        "       (\"mkt_id: 128374|user-click|product_page\",)]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data, [\"mkt_code\"])\n",
        "df.show(truncate=False)\n",
        "\n",
        "\n",
        "# Apply transformations\n",
        "transformed_df = df.withColumn(\"product_marketing_id\",\n",
        "                              expr(\"REPLACE(REGEXP_EXTRACT(mkt_code, 'mkt_id:[^|]*', 0), 'mkt_id:', '')\"))\n",
        "\n",
        "# Extract the user-click value using regexp_extract\n",
        "\n",
        "\n",
        "#transformed_df = transformed_df.withColumn(\"product_view\",regexp_extract(col(\"mkt_code\"), '\\|(.*?)\\|', 1))\n",
        "#transformed_df = transformed_df.withColumn(\"product_page\",regexp_extract(col(\"mkt_code\"), '\\|[-_a-z]+\\|(.*?)$', 1))\n",
        "# Show transformed DataFrame\n",
        "#transformed_df.show(truncate=False)\n",
        "split_df = transformed_df.withColumn(\"split_array\", split(\"mkt_code\", \"\\|\"))\n",
        "\n",
        "# Extract the landing_page value using element_at (last element)\n",
        "extracted_df = split_df.withColumn(\"product_view\", element_at(\"split_array\", 2))\n",
        "extracted_df = extracted_df.withColumn(\"product_page\", element_at(\"split_array\", -1))  # -1 for last element\n",
        "\n",
        "# Show transformed DataFrame\n",
        "extracted_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlTUeNnG6aKA",
        "outputId": "5bd20586-62f4-451f-f2ae-8db7f67e7764"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------+\n",
            "|mkt_code                              |\n",
            "+--------------------------------------+\n",
            "|mkt_id: 736482|user-click|landing_page|\n",
            "|mkt_id: 839274|user-view|homepage     |\n",
            "|mkt_id: 128374|user-click|product_page|\n",
            "+--------------------------------------+\n",
            "\n",
            "+--------------------------------------+--------------------+------------------------------------------+------------+------------+\n",
            "|mkt_code                              |product_marketing_id|split_array                               |product_view|product_page|\n",
            "+--------------------------------------+--------------------+------------------------------------------+------------+------------+\n",
            "|mkt_id: 736482|user-click|landing_page| 736482             |[mkt_id: 736482, user-click, landing_page]|user-click  |landing_page|\n",
            "|mkt_id: 839274|user-view|homepage     | 839274             |[mkt_id: 839274, user-view, homepage]     |user-view   |homepage    |\n",
            "|mkt_id: 128374|user-click|product_page| 128374             |[mkt_id: 128374, user-click, product_page]|user-click  |product_page|\n",
            "+--------------------------------------+--------------------+------------------------------------------+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que 5\n",
        "To find the start and end flight from the dataset"
      ],
      "metadata": {
        "id": "Qn_OGQguNkCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CreateDataFrameExample\").getOrCreate()\n",
        "\n",
        "data = [ ('c1', 'New York', 'Lima'),\n",
        "   ('c1', 'London', 'New York'),\n",
        "   ('c1', 'Lima', 'Sao Paulo'),\n",
        "   ('c1', 'Sao Paulo', 'New Delhi'),\n",
        "   ('c2', 'Mumbai', 'Hyderabad'),\n",
        "   ('c2', 'Surat', 'Pune'),\n",
        "   ('c2', 'Hyderabad', 'Surat'),\n",
        "   ('c3', 'Kochi', 'Kurnool'),\n",
        "   ('c3', 'Lucknow', 'Agra'),\n",
        "   ('c3', 'Agra', 'Jaipur'),\n",
        "   ('c3', 'Jaipur', 'Kochi')]\n",
        "\n",
        "schema = \"customer string , start_location string , end_location string\"\n",
        "df = spark.createDataFrame(data = data , schema = schema)\n",
        "df.show()\n",
        "\n",
        "df_start = df.select(col(\"customer\"), col(\"start_location\").alias(\"loc\"))\n",
        "df_end = df.select(col(\"customer\"), col(\"end_location\").alias(\"loc\"))\n",
        "\n",
        "df_union = df_start.union(df_end)\n",
        "\n",
        "df_unique = df_union.groupBy(col(\"customer\") , col(\"loc\")).agg(\n",
        "   count(lit(1)).alias(\"cn\")\n",
        ").filter(col(\"cn\") ==1).drop(col(\"cn\")).orderBy(col(\"customer\"))\n",
        "\n",
        "df_unique.show()\n",
        "df_answer = df.join(df_unique, on= ((df.customer == df_unique.customer) &(df.start_location == df_unique.loc) | (df.end_location == df_unique.loc)), how='inner').drop(df.customer)\n",
        "df_answer = df_answer.withColumn('cust_start_loc', when(col('start_location')==col('loc'), col('start_location'))).withColumn('cust_end_loc', when(col('end_location')==col('loc'), col('end_location'))).select(col(\"customer\"), col(\"cust_start_loc\"), col(\"cust_end_loc\"))\n",
        "df_answer.show()\n",
        "df_answer = df_answer.groupby(col('customer')).agg(min(col('cust_start_loc')).alias('start_loation'),min(col('cust_end_loc')).alias('end_loation'))\n",
        "df_answer.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwQ61-Xo-psB",
        "outputId": "a24d1a53-b790-44c9-88e7-ff184b27de44"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------+------------+\n",
            "|customer|start_location|end_location|\n",
            "+--------+--------------+------------+\n",
            "|      c1|      New York|        Lima|\n",
            "|      c1|        London|    New York|\n",
            "|      c1|          Lima|   Sao Paulo|\n",
            "|      c1|     Sao Paulo|   New Delhi|\n",
            "|      c2|        Mumbai|   Hyderabad|\n",
            "|      c2|         Surat|        Pune|\n",
            "|      c2|     Hyderabad|       Surat|\n",
            "|      c3|         Kochi|     Kurnool|\n",
            "|      c3|       Lucknow|        Agra|\n",
            "|      c3|          Agra|      Jaipur|\n",
            "|      c3|        Jaipur|       Kochi|\n",
            "+--------+--------------+------------+\n",
            "\n",
            "+--------+---------+\n",
            "|customer|      loc|\n",
            "+--------+---------+\n",
            "|      c1|   London|\n",
            "|      c1|New Delhi|\n",
            "|      c2|   Mumbai|\n",
            "|      c2|     Pune|\n",
            "|      c3|  Lucknow|\n",
            "|      c3|  Kurnool|\n",
            "+--------+---------+\n",
            "\n",
            "+--------+--------------+------------+\n",
            "|customer|cust_start_loc|cust_end_loc|\n",
            "+--------+--------------+------------+\n",
            "|      c1|        London|        NULL|\n",
            "|      c1|          NULL|   New Delhi|\n",
            "|      c2|        Mumbai|        NULL|\n",
            "|      c2|          NULL|        Pune|\n",
            "|      c3|          NULL|     Kurnool|\n",
            "|      c3|       Lucknow|        NULL|\n",
            "+--------+--------------+------------+\n",
            "\n",
            "+--------+-------------+-----------+\n",
            "|customer|start_loation|end_loation|\n",
            "+--------+-------------+-----------+\n",
            "|      c1|       London|  New Delhi|\n",
            "|      c2|       Mumbai|       Pune|\n",
            "|      c3|      Lucknow|    Kurnool|\n",
            "+--------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dghDBIwfH-Qb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}